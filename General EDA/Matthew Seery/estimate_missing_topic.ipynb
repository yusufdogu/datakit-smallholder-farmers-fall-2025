{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edad6b0-81ca-458f-871a-f7488027e29d",
   "metadata": {},
   "source": [
    "# PREDICT VALUE FOR MISSING QUESTION TOPIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a5ef8-99bd-43a8-83a0-b4c184034fb0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c538316c-9abf-479f-8e67-0b56afd4637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 15:31:04.965149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763613065.031574     555 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763613065.051774     555 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1763613065.421332     555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763613065.421377     555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763613065.421380     555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763613065.421382     555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_MIN_GPU_MULTIPROCESSOR_COUNT'] = '6' # Needed so I can use my old GPU with the new one\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' # Turns off oneDNN custom operations\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # Hides message regarding TensorFlow optimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2991ae-3112-4dde-8da6-0d5b7daa2d32",
   "metadata": {},
   "source": [
    "## Import CSV File & Remove Duplicates\n",
    "NB:\n",
    "\n",
    "'question_topic_valid.csv' represents records from the original dataset where 'question_topic' was not empty while 'question_topic_null.csv' are records with missing values for 'question_topic'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbb4951-125c-42ab-9767-4b03b73c326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import patitioned libraries of original dataset \n",
    "df_topic_exists = pd.read_csv('../data/question_topic_valid.csv', usecols=[2,3,4,13,14]) # Import only essential columns\n",
    "df_topic_null = pd.read_csv('../data/question_topic_null.csv')\n",
    "\n",
    "# Drops duplicate values for 'question_content' that were created in the original dataset because of the 'response' columns\n",
    "df_topic_exists.drop_duplicates(inplace=True)\n",
    "df_topic_exists.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafbd64-c133-4cf5-b57a-e331a75582b7",
   "metadata": {},
   "source": [
    "## Tokenize The Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243fcdda-d3c0-4493-bfe0-09c99ac6a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_sentences(col):\n",
    "    sentence_list = []\n",
    "    for text in col:\n",
    "        splitted_text = text.lower().split()\n",
    "        sentence_list.append(splitted_text)\n",
    "    return sentence_list\n",
    "\n",
    "sentences_topic_exist = list_of_sentences(df_topic_exists.question_content)\n",
    "sentences_topic_null = list_of_sentences(df_topic_null.question_content)\n",
    "\n",
    "# init the tokenizer with a out_of_vocabulary token \n",
    "tokenizer_X = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "# generate word indexes for all sentences \n",
    "tokenizer_X.fit_on_texts(sentences_topic_exist+sentences_topic_null)\n",
    "\n",
    "# generate separate sequences for both with topic values and missing values\n",
    "X = tokenizer_X.texts_to_sequences(sentences_topic_exist)\n",
    "X_topic_null = tokenizer_X.texts_to_sequences(sentences_topic_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6956b-9f6c-4a3d-8767-807e1aeb5ce1",
   "metadata": {},
   "source": [
    "## Determine Word Counts & Maximum Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f803052c-b3ca-4469-92e3-bd784941bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words from all questions is 1292953.\n",
      "The highest number of words in any sentence is 197.\n"
     ]
    }
   ],
   "source": [
    "print(f'The total number of words from all questions is {len(tokenizer_X.word_counts)}.')\n",
    "\n",
    "max_len = 0\n",
    "for l in X + X_topic_null:\n",
    "    if len(l) > max_len:\n",
    "        max_len = len(l)\n",
    "\n",
    "print(f'The highest number of words in any sentence is {max_len}.')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "580cc7af-4d7e-4341-99a7-81ff5e4b1f3e",
   "metadata": {},
   "source": [
    "Total number of words for 'sentences' (no missing values for 'question_topic') is 891139.\n",
    "\n",
    "Total number of words for 'sentences_topic_null' is 692022.\n",
    "\n",
    "For 'sentences_topic_null' there are 401814 words (1292953 - 891139) that do not appear in 'sentences'.\n",
    "\n",
    "Hence there are 290208 words (692022 - 401814) that appear in both 'sentences' and 'sentences_topic_null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4942d2bf-9b4c-4dd7-9e4f-240485a8d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000     # Use 20000 most frequent words from the total of 1292953 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5993d23-c057-4b78-b556-5cd8d536268b",
   "metadata": {},
   "source": [
    "## Create Train & Test Datasets & Prepare For Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315e1535-0d21-40bb-8425-885f79bc7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, df_topic_exists.question_topic, test_size=0.2,\n",
    "                                                                stratify=df_topic_exists.question_topic, random_state=42)\n",
    "\n",
    "# Format X and y for model\n",
    "X_train = np.array(sequence.pad_sequences(X_train_df, maxlen=max_len))\n",
    "X_test = np.array(sequence.pad_sequences(X_test_df, maxlen=max_len))\n",
    "\n",
    "y_train_one_hot = pd.get_dummies(y_train_df)\n",
    "y_train = y_train_one_hot.to_numpy()\n",
    "y_test_one_hot = pd.get_dummies(y_test_df)\n",
    "y_test = y_test_one_hot.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb8495-742f-4d35-ab60-7ffd05c21821",
   "metadata": {},
   "source": [
    "## Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f680d6e-d724-4ba7-b20d-fdd862fbf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer block class\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e8d561-5861-4b89-86e1-c73ef6064639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763613477.288486     555 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5518 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1763613477.291889     555 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2857 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Define the model with an embedding layer, transformer block, and output layer\n",
    "embed_dim = 32 # embedding dimension for each word vector\n",
    "num_heads = 2  # the number of attention heads in the multi-head attention layer\n",
    "ff_dim = 32    # number of units in the feed forward layer\n",
    "\n",
    "inputs = layers.Input(shape=(max_len,))\n",
    "\n",
    "embedding_layer = layers.Embedding(input_dim=max_features, output_dim=embed_dim)\n",
    "out = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "out = transformer_block(out, training=True)\n",
    "out = layers.GlobalAveragePooling1D()(out)\n",
    "out = layers.Dropout(0.1)(out)\n",
    "out = layers.Dense(20, activation='relu')(out)\n",
    "out = layers.Dropout(0.1)(out)\n",
    "outputs = layers.Dense(148, activation='softmax')(out)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73569ba3-a84f-4fe5-b6cc-0d6145c09d6a",
   "metadata": {},
   "source": [
    "## Compile & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945f82e3-e324-47d5-a88e-835800da38c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 15:38:02.293059: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2318536340 exceeds 10% of free system memory.\n",
      "2025-11-20 15:38:05.934049: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 435461140 exceeds 10% of free system memory.\n",
      "2025-11-20 15:38:07.584169: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 435461140 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763613498.396410     831 service.cc:152] XLA service 0x72892c009b30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1763613498.406317     831 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "I0000 00:00:1763613498.406343     831 service.cc:160]   StreamExecutor device (1): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "I0000 00:00:1763613500.283499     831 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1763613520.246825     831 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5738 - loss: 1.7989"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 15:40:52.506728: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 579634676 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 53ms/step - accuracy: 0.7362 - loss: 1.0314 - val_accuracy: 0.8242 - val_loss: 0.5198\n",
      "Epoch 2/3\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 42ms/step - accuracy: 0.8166 - loss: 0.5726 - val_accuracy: 0.8257 - val_loss: 0.4836\n",
      "Epoch 3/3\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 43ms/step - accuracy: 0.8194 - loss: 0.5316 - val_accuracy: 0.8263 - val_loss: 0.4615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x728d8a6ec350>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=1024, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17dd374-2685-4c79-86ea-3e9bdc6407f5",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54c3f34-1aeb-4524-bcb5-92f63cd4f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 15:45:27.078871: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 724543148 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28734/28734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 6ms/step - accuracy: 0.8271 - loss: 0.4597\n",
      "Test Accuracy: 0.8271299600601196\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83c435-514d-414d-8502-f4137e57f446",
   "metadata": {},
   "source": [
    "## Extract Failed Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "807fcf2b-18d8-4695-9d1d-ce6a273290a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28734/28734\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Store in a list the column names for one-hot encoding (question_topic)\n",
    "one_hot_columns = list(y_test_one_hot.columns)\n",
    "\n",
    "# Store predictions for X_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Add predictions column to y_test_df\n",
    "y_test_df = y_test_df.to_frame()\n",
    "y_test_df['predictions'] = [one_hot_columns[i] for i in np.argmax(y_pred, axis=1)]\n",
    "\n",
    "# Merge index associated rows from the original source dataset along with the predictions \n",
    "test_df = pd.merge(df_topic_exists, y_test_df, left_index=True, right_index=True)\n",
    "\n",
    "# Create new dataframe that stores rows from test df where predictions were incorrect plus adds the predictions column\n",
    "false_predictions = pd.DataFrame()\n",
    "for i,v in test_df.iterrows():\n",
    "    if v.question_topic_x != v.predictions:\n",
    "        row = pd.DataFrame({'question_language' : [v.question_language], 'question_content' : [v.question_content],\n",
    "                            'question_user_status' : [v.question_user_status], 'question_user_country_code' : [v.question_user_country_code],\n",
    "                            'question_topic': [v.question_topic_x], 'predictions' : [v.predictions]\n",
    "                            })\n",
    "        false_predictions = pd.concat([false_predictions, row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01507607-286b-4f4b-94a4-b8d4add2b628",
   "metadata": {},
   "source": [
    "# Export Test df For Predictions Versus Actual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02586d05-fa9f-4a02-8847-fb6e5faaf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../data/prediction_vs_actual_topic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace49208-9f2c-4afe-b1f7-78da0638fc4a",
   "metadata": {},
   "source": [
    "## Check For Any Indicators For Failure Rate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92605c71-b205-4886-9f8c-8a5b2ce50567",
   "metadata": {},
   "source": [
    "The overall failure rate for the test data set is 17.3%.\n",
    "\n",
    "For question_language, question_user_country_code, and question_user_status, the percentage failure rates are fairly consistent with the overall failure rate.\n",
    "\n",
    "For question_topic, topics with the lowest counts tended to have very high failure rates presumably because there were not enough samples to learn from. The one noticeable exception to this was 'plant' which had a failure rate of 49.05% despite there being 44020 samples in the test dataset. Possibly 'plant' is too generic given that there are other plants listed in the topics.\n",
    "\n",
    "The topics in question_topic with lower failure rates tended to have higher counts although there were some values that performed well despite the very small sample size. The topic 'rabbit' performed best with only 4.65% failure rate. There were 25 topics with a sample size less than 100 that could not make even 1 correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cac0a96-50a4-45de-a6f6-5ec092e79ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test data % failure rate by question_language\n",
      "eng    18.167660\n",
      "swa    16.958074\n",
      "nyn    11.906100\n",
      "lug    15.306250\n",
      "Name: count, dtype: float64\n",
      "\n",
      "The test data % failure rate by question_user_country_code\n",
      "ke    19.857620\n",
      "ug    16.139403\n",
      "tz    13.898236\n",
      "gb    23.529412\n",
      "Name: count, dtype: float64\n",
      "\n",
      "The test data % failure rate by question_user_status\n",
      "live         16.862772\n",
      "zombie       18.568636\n",
      "destroyed    18.450623\n",
      "blocked      16.258896\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f'The test data % failure rate by {(false_predictions.question_language.value_counts() / test_df.question_language.value_counts()) * 100}\\n')\n",
    "print(f'The test data % failure rate by {(false_predictions.question_user_country_code.value_counts() / test_df.question_user_country_code.value_counts()) * 100}\\n')\n",
    "print(f'The test data % failure rate by {(false_predictions.question_user_status.value_counts() / test_df.question_user_status.value_counts()) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3681182e-40b8-4790-8532-e3188a7d8da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 60 failure rates \n",
      "       question_topic  failed_prediction  total  percentage_failed\n",
      "0          courgette                 56     56         100.000000\n",
      "1        castor-bean                  7      7         100.000000\n",
      "2              vetch                  7      7         100.000000\n",
      "3       purple-vetch                  2      2         100.000000\n",
      "4          caliandra                 13     13         100.000000\n",
      "5          asparagus                 12     12         100.000000\n",
      "6             celery                 11     11         100.000000\n",
      "7           snap-pea                 10     10         100.000000\n",
      "8            apricot                  8      8         100.000000\n",
      "9               leek                 17     17         100.000000\n",
      "10          chickpea                 26     26         100.000000\n",
      "11        gooseberry                 30     30         100.000000\n",
      "12           setaria                  5      5         100.000000\n",
      "13          leucaena                  6      6         100.000000\n",
      "14             lupin                  6      6         100.000000\n",
      "15        blackberry                  5      5         100.000000\n",
      "16             chard                 24     24         100.000000\n",
      "17         cranberry                  1      1         100.000000\n",
      "18          mulberry                  4      4         100.000000\n",
      "19             peach                186    186         100.000000\n",
      "20               rye                  6      6         100.000000\n",
      "21  black-nightshade                 45     45         100.000000\n",
      "22        guinea-pig                 53     53         100.000000\n",
      "23              flax                 80     80         100.000000\n",
      "24            sesame                 76     76         100.000000\n",
      "25           lettuce                 33     34          97.058824\n",
      "26              pear                317    382          82.984293\n",
      "27     finger-millet                183    228          80.263158\n",
      "28         safflower                182    227          80.176211\n",
      "29            clover                164    205          80.000000\n",
      "30             guava                301    391          76.982097\n",
      "31      napier-grass               1975   2653          74.444026\n",
      "32            garlic                720    996          72.289157\n",
      "33            cereal               3217   4651          69.167921\n",
      "34            acacia                120    176          68.181818\n",
      "35            barley                 59     87          67.816092\n",
      "36          plantain               3608   5460          66.080586\n",
      "37            greens                273    414          65.942029\n",
      "38          amaranth                103    157          65.605096\n",
      "39             sisal                243    401          60.598504\n",
      "40             wheat               3951   6833          57.822333\n",
      "41          rapeseed                 77    134          57.462687\n",
      "42            chilli               1213   2125          57.082353\n",
      "43            radish                 64    116          55.172414\n",
      "44               oat                 44     81          54.320988\n",
      "45          snow-pea                116    223          52.017937\n",
      "46         desmodium                 64    127          50.393701\n",
      "47            squash                132    262          50.381679\n",
      "48             lemon                167    332          50.301205\n",
      "49               cat                754   1514          49.801849\n",
      "50             plant              21592  44020          49.050432\n",
      "51    collard-greens                 81    166          48.795181\n",
      "52      sweet-potato               1906   4032          47.271825\n",
      "53          broccoli                 18     39          46.153846\n",
      "54            pigeon                814   1826          44.578313\n",
      "55       cauliflower                 14     32          43.750000\n",
      "56           parsley                108    252          42.857143\n",
      "57             olive                500   1234          40.518639\n",
      "58           coconut                167    413          40.435835\n",
      "59         jackfruit                 85    216          39.351852\n",
      "\n",
      "The bottom 60 failure rates \n",
      "        question_topic  failed_prediction   total  percentage_failed\n",
      "88              mango                633    2837          22.312302\n",
      "89               bird                902    4072          22.151277\n",
      "90               soya                454    2051          22.135544\n",
      "91          pyrethrum                 53     240          22.083333\n",
      "92   butternut-squash                147     668          22.005988\n",
      "93        guinea-fowl                331    1554          21.299871\n",
      "94              grape                 26     123          21.138211\n",
      "95                pea                410    1946          21.068859\n",
      "96          aubergine                405    1954          20.726714\n",
      "97               tree               1643    8121          20.231499\n",
      "98               bean               7929   39896          19.874173\n",
      "99              miraa                 63     322          19.565217\n",
      "100            millet               1050    5444          19.287289\n",
      "101           chicken              17055   93143          18.310555\n",
      "102         livestock               1399    7972          17.548921\n",
      "103         pineapple                263    1517          17.336849\n",
      "104            potato               3968   22977          17.269443\n",
      "105           cassava                998    5787          17.245550\n",
      "106           poultry              10488   61397          17.082268\n",
      "107       french-bean                101     605          16.694215\n",
      "108              goat               4240   25896          16.373185\n",
      "109            peanut               1286    7869          16.342610\n",
      "110          mushroom                229    1406          16.287340\n",
      "111          capsicum                533    3330          16.006006\n",
      "112           avocado                495    3116          15.885751\n",
      "113           spinach                323    2069          15.611406\n",
      "114              duck                821    5359          15.320022\n",
      "115        pigeon-pea                 77     503          15.308151\n",
      "116               yam                115     766          15.013055\n",
      "117              crop               2953   19839          14.884823\n",
      "118             sheep               1277    8844          14.439168\n",
      "119               pig               3547   25496          13.911986\n",
      "120             cocoa                 90     657          13.698630\n",
      "121           pumpkin                178    1319          13.495072\n",
      "122     passion-fruit                984    7478          13.158599\n",
      "123            cotton               1300    9942          13.075840\n",
      "124              kale                792    6226          12.720848\n",
      "125        sugar-cane                677    5363          12.623532\n",
      "126               bee                745    5930          12.563238\n",
      "127        strawberry                 28     226          12.389381\n",
      "128        cashew-nut                142    1209          11.745244\n",
      "129           tobacco                122    1088          11.213235\n",
      "130            tomato               7356   66336          11.089001\n",
      "131            banana               1594   15860          10.050441\n",
      "132              fish                486    5019           9.683204\n",
      "133            coffee               1759   18173           9.679194\n",
      "134              chia                 19     199           9.547739\n",
      "135             onion               1561   17236           9.056626\n",
      "136            ginger                135    1491           9.054326\n",
      "137             maize               9819  109778           8.944415\n",
      "138             melon                288    3230           8.916409\n",
      "139            carrot                344    3987           8.628041\n",
      "140         sunflower                290    3397           8.536944\n",
      "141               dog                277    3295           8.406677\n",
      "142               tea                396    4927           8.037345\n",
      "143             apple                 89    1110           8.018018\n",
      "144           cabbage               1024   13267           7.718399\n",
      "145              rice                969   15315           6.327130\n",
      "146            cattle               5057   86877           5.820873\n",
      "147            rabbit                774   16628           4.654799\n"
     ]
    }
   ],
   "source": [
    "question_topic_failed = false_predictions.question_topic.value_counts().rename_axis('question_topic').reset_index(name='failed_prediction')\n",
    "question_topic_total = test_df.question_topic_x.value_counts().rename_axis('question_topic').reset_index(name='total')\n",
    "question_topic = pd.merge(question_topic_failed, question_topic_total, how='inner')\n",
    "question_topic['percentage_failed'] = (question_topic['failed_prediction'] / question_topic['total']) * 100\n",
    "question_topic = question_topic.sort_values(by=['percentage_failed'],ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f'The top 60 failure rates \\n {question_topic.head(60)}\\n')\n",
    "print(f'The bottom 60 failure rates \\n {question_topic.tail(60)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a48a3-5e29-4bc9-ae29-c85c19c49387",
   "metadata": {},
   "source": [
    "# Free Memory For Next Step\n",
    "\n",
    "NB: Optional step if system resources are limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec2735ac-115c-478a-b46d-6c770a11f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %xdel sentences_topic_exist\n",
    "# %xdel X \n",
    "# %xdel X_train_df\n",
    "# %xdel X_test_df\n",
    "# %xdel y_train_df\n",
    "# %xdel y_test_df\n",
    "# %xdel X_train\n",
    "# %xdel X_test\n",
    "# %xdel y_train_one_hot\n",
    "# %xdel y_train\n",
    "# %xdel y_test_one_hot\n",
    "# %xdel y_test\n",
    "# %xdel y_pred\n",
    "# %xdel false_predictions\n",
    "# %xdel df_topic_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a5ba5-a931-45b1-bbd4-096fc8d4c008",
   "metadata": {},
   "source": [
    "## Make Predictions For Missing question_topic Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2052611-2997-4b2a-b43e-da411a9cf572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110555/110555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create X input and make predictions\n",
    "X_topic_null_predict = np.array(sequence.pad_sequences(X_topic_null, maxlen=max_len))\n",
    "y_pred_topic_null = model.predict(X_topic_null_predict)\n",
    "\n",
    "# Convert predictions to labels\n",
    "topic_null_predictions = [one_hot_columns[i] for i in np.argmax(y_pred_topic_null, axis=1)]\n",
    "\n",
    "# Insert predictions into 'question_topic' column for null dataframe\n",
    "df_topic_null['question_topic'] = topic_null_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0555e7d-66bb-4a23-9162-e167b3c35d03",
   "metadata": {},
   "source": [
    "# Free Memory For Next Step\n",
    "\n",
    "NB: Optional step if system resources are limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f653521-2b59-4fec-b162-158963a4a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %xdel X_topic_null_predict\n",
    "# %xdel y_pred_topic_null\n",
    "# %xdel topic_null_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b97247-398a-40fe-856e-603baa6ce01f",
   "metadata": {},
   "source": [
    "# Export To CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccf1aa7d-d629-4264-b110-54dee3e903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datset without missing values for 'question_topic'\n",
    "chunks = pd.read_csv('../data/question_topic_valid.csv',\n",
    "                     dtype={'question_user_gender': str, 'response_user_gender': str}, # Removes mixed dtypes error message\n",
    "                     chunksize=100000\n",
    "                    )\n",
    "\n",
    "df_topic_exists = pd.DataFrame()\n",
    "for chunk in chunks:\n",
    "    df_topic_exists = pd.concat([df_topic_exists,chunk], axis=0)\n",
    "\n",
    "\n",
    "# Combine dataset without missing values with the predicted values to recreate the full dataset\n",
    "df_no_missing = pd.concat([df_topic_exists, df_topic_null], axis=0)\n",
    "\n",
    "\n",
    "# Export the predicted values only and the full dataset now with no missing values\n",
    "df_topic_null.to_csv('../data/question_topic_predicted.csv', index=False)\n",
    "df_no_missing.to_csv('../data/question_topic_no_missing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33932dc4-4b15-4477-bd54-af5b619db12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
